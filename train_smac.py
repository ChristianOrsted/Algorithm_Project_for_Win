from weak_tie_env import WeakTieStarCraft2Env
import numpy as np
import time
import os
import sys
import glob
from datetime import datetime
from weak_tie_module import WeakTieGraph
# === å¼•å…¥æ›´æ–°åçš„ Agent ç±» ===
from weak_tie_agent import WeakTieAgent
import torch

# ==============================================================================
# é…ç½®å‚æ•°
# ==============================================================================
MAP_NAME = "1c3s5z"
N_EPISODES = 100000
BATCH_SIZE = 64
# ç”±äº Strict Mode è®¡ç®—é‡å¤§ï¼Œå¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œè¯·å°†æ­¤å€¼è°ƒå°ä¸º 8
MINI_BATCH_SIZE = 32
PPO_EPOCH = 10
OBS_RANGE = 15.0
EVAL_INTERVAL = 500
# === [ä¿®æ”¹] å°†è¯„ä¼°æ¬¡æ•°ä» 20 æå‡åˆ° 50 ===
EVAL_EPISODES = 50
MODEL_PATH = "best_model.pt"
CHECKPOINT_DIR = "checkpoints"
CHECKPOINT_INTERVAL = 1000  # æ¯ 1000 è½®è¦†ç›–æ›´æ–°ä¸€æ¬¡å­˜æ¡£
RESUME_SOURCE = "ckpt"  # å¯é€‰: "ckpt", "best", "latest", "none"

# æé€Ÿä¼˜åŒ–
GRAPH_UPDATE_INTERVAL = 3

# Entropy Decay
ENTROPY_START = 0.02
ENTROPY_END = 0.001
ENTROPY_DECAY_EPISODES = 20000

if MAP_NAME in ["1c3s5z", "50m", "10m_vs_11m"]:
    HIDDEN_DIM = 256
    LR = 0.0003
else:
    HIDDEN_DIM = 128
    LR = 0.0005


# ==============================================================================
# æ—¥å¿—ç³»ç»Ÿ
# ==============================================================================
class Logger:
    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log = open(log_file, 'a', encoding='utf-8')

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()

    def flush(self):
        self.terminal.flush()
        self.log.flush()


def setup_logger(log_dir='log'):
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    log_file = os.path.join(log_dir, f'training_log_{MAP_NAME}.txt')
    logger = Logger(log_file)
    sys.stdout = logger
    sys.stderr = logger
    print(f"\n{'=' * 60}")
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"è®­ç»ƒåœ°å›¾: {MAP_NAME} | ç›®æ ‡å›åˆ: {N_EPISODES}")
    print(f"{'=' * 60}\n")
    return log_file


def get_current_entropy(episode):
    if episode > ENTROPY_DECAY_EPISODES:
        return ENTROPY_END
    frac = 1.0 - (episode / ENTROPY_DECAY_EPISODES)
    return max(ENTROPY_END, ENTROPY_END + frac * (ENTROPY_START - ENTROPY_END))


def peek_model_info(path, device):
    """åªè¯»å–æ¨¡å‹æ–‡ä»¶ä¸­çš„ episode å’Œ win_rate ä¿¡æ¯ï¼Œä¸åŠ è½½å‚æ•°"""
    if not os.path.exists(path):
        return None, 0.0
    try:
        # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  weights_only=False
        ckpt = torch.load(path, map_location=device, weights_only=False)
        ep = ckpt.get('episode', 0)
        wr = ckpt.get('win_rate', 0.0)
        return ep, wr
    except Exception as e:
        print(f"æ— æ³•è¯»å–æ–‡ä»¶ {path}: {e}")
        return None, 0.0


def run_episode(env, agent, wt_graph, train_mode=True):
    obs, state = env.reset()
    terminated = False
    episode_reward = 0
    raw_episode_reward = 0

    actor_hidden = agent.init_hidden(batch_size=1)

    episode_buffer = {'obs': [], 'acts': [], 'rewards': [], 'dones': [],
                      'avails': [], 'probs': [], 'masks': [], 'keys': []}

    step_count = 0
    last_mask_beta = None
    last_key_agent_idx = None

    while not terminated:
        avail_actions = env.get_avail_actions()
        positions = env.get_all_unit_positions()
        alive_mask = np.any(positions != 0, axis=1)

        # åŠ¨æ€å›¾æ›´æ–°
        if step_count % GRAPH_UPDATE_INTERVAL == 0:
            mask_beta, key_agent_idx = wt_graph.compute_graph_info(positions, alive_mask)
            last_mask_beta = mask_beta
            last_key_agent_idx = key_agent_idx
        else:
            mask_beta = last_mask_beta
            key_agent_idx = last_key_agent_idx

        step_count += 1

        actions, probs, next_hidden = agent.select_action(
            obs, avail_actions, mask_beta, key_agent_idx, actor_hidden,
            deterministic=(not train_mode)
        )

        reward, terminated, info = env.step(actions)
        next_obs = env.get_obs()

        shaped_reward = reward / 5.0

        if train_mode:
            episode_buffer['obs'].append([obs])
            episode_buffer['acts'].append([actions])
            episode_buffer['rewards'].append([[shaped_reward] * len(actions)])
            episode_buffer['dones'].append([[float(terminated)] * len(actions)])
            episode_buffer['avails'].append([avail_actions])
            episode_buffer['probs'].append([probs])
            episode_buffer['masks'].append([mask_beta])
            episode_buffer['keys'].append([key_agent_idx])

        obs = next_obs
        actor_hidden = next_hidden
        episode_reward += shaped_reward
        raw_episode_reward += reward

    is_win = info.get('battle_won', False)
    return episode_reward, raw_episode_reward, is_win, episode_buffer, None


def main():
    log_file = setup_logger()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")

    try:
        env = WeakTieStarCraft2Env(map_name=MAP_NAME, difficulty="7")
    except Exception as e:
        print(f"ç¯å¢ƒå¯åŠ¨å¤±è´¥: {e}")
        return

    env_info = env.get_env_info()
    n_agents = env_info["n_agents"]
    n_actions = env_info["n_actions"]
    obs_dim = env_info["obs_shape"]

    wt_graph = WeakTieGraph(n_agents, obs_range=OBS_RANGE, alpha_quantile=0.3)

    # å®ä¾‹åŒ– Agent
    agent = WeakTieAgent(n_agents, obs_dim, n_actions,
                         hidden_dim=HIDDEN_DIM, lr=LR,
                         ppo_epoch=PPO_EPOCH, mini_batch_size=MINI_BATCH_SIZE)

    # ==========================================================================
    # ğŸ” æ™ºèƒ½æ¨¡å‹åŠ è½½é€»è¾‘
    # ==========================================================================
    ckpt_path = os.path.join(CHECKPOINT_DIR, "ckpt_latest.pt")
    best_path = MODEL_PATH

    # 1. ä¾¦å¯Ÿï¼šçœ‹çœ‹ç°åœ¨ç¡¬ç›˜é‡Œæœ‰å“ªäº›å­˜æ¡£
    ckpt_ep, _ = peek_model_info(ckpt_path, device)
    best_ep, saved_best_win_rate = peek_model_info(best_path, device) # è¯»å–å†å²æœ€ä½³èƒœç‡

    print(f"\nå­˜æ¡£çŠ¶æ€æ‰«æ:")
    print(f"   [Ckpt]  è‡ªåŠ¨å­˜æ¡£: {'ä¸å­˜åœ¨' if ckpt_ep is None else f'Ep {ckpt_ep}'}")
    print(f"   [Best]  æœ€ä½³æ¨¡å‹: {'ä¸å­˜åœ¨' if best_ep is None else f'Ep {best_ep} (WinRate: {saved_best_win_rate:.1%})'}")

    start_episode = 0
    target_file = None

    # 2. å†³ç­–ï¼šæ ¹æ® RESUME_SOURCE å†³å®šåŠ è½½è°
    if RESUME_SOURCE == "ckpt":
        if ckpt_ep is not None:
            target_file = ckpt_path
            print(f"ç­–ç•¥: å¼ºåˆ¶åŠ è½½ Ckpt")
        elif best_ep is not None:
            target_file = best_path
            print(f"ç­–ç•¥è¦æ±‚åŠ è½½ Ckpt ä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•åŠ è½½ Best Modelã€‚")

    elif RESUME_SOURCE == "best":
        if best_ep is not None:
            target_file = best_path
            print(f"ç­–ç•¥: å¼ºåˆ¶åŠ è½½ Best Model")
        elif ckpt_ep is not None:
            target_file = ckpt_path
            print(f"ç­–ç•¥è¦æ±‚åŠ è½½ Best ä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•åŠ è½½ Ckptã€‚")

    elif RESUME_SOURCE == "latest":
        print(f"ç­–ç•¥: è‡ªåŠ¨é€‰æ‹©è½®æ•°æœ€æ–°çš„æ¨¡å‹")
        ep_c = ckpt_ep if ckpt_ep is not None else -1
        ep_b = best_ep if best_ep is not None else -1

        if ep_c > ep_b:
            target_file = ckpt_path
        elif ep_b > -1:
            target_file = best_path

    elif RESUME_SOURCE == "none":
        print(f"ç­–ç•¥: å¼ºåˆ¶ä»å¤´å¼€å§‹è®­ç»ƒã€‚")

    # 3. æ‰§è¡ŒåŠ è½½
    if target_file:
        print(f"æœ€ç»ˆå†³å®šåŠ è½½: {target_file}")
        start_episode = agent.load_model(target_file)
    else:
        print(f"æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹æˆ–ç­–ç•¥è®¾ä¸º noneï¼Œä»å¤´å¼€å§‹è®­ç»ƒã€‚")

    if start_episode >= N_EPISODES:
        print("è®­ç»ƒç›®æ ‡å·²è¾¾æˆï¼Œæ— éœ€ç»§ç»­è®­ç»ƒã€‚")
        env.close()
        return

    # ==========================================================================

    # === [å…³é”®] åˆå§‹åŒ– best_win_rate ä¸ºå†å²è®°å½•çš„å€¼ ===
    # å¦‚æœæ˜¯åˆæ¬¡è®­ç»ƒï¼Œåˆ™ä¸º 0.0ï¼›å¦‚æœæ˜¯æ¥ç»­è®­ç»ƒï¼Œåˆ™ç»§æ‰¿ä¹‹å‰çš„è®°å½•
    best_win_rate = saved_best_win_rate if saved_best_win_rate is not None else 0.0
    print(f"å½“å‰ä¼šè¯èµ·å§‹æœ€ä½³èƒœç‡: {best_win_rate:.1%}")

    total_wins = 0
    recent_raw_rewards = []
    batch_buffer = []

    training_start_time = time.time()

    print(f"\næ­£å¼å¼€å§‹è®­ç»ƒ (ä» Ep {start_episode + 1} åˆ° {N_EPISODES})...\n")

    for episode in range(start_episode + 1, N_EPISODES + 1):
        curr_entropy = get_current_entropy(episode)

        _, raw_reward, is_win, buffer, _ = run_episode(env, agent, wt_graph, train_mode=True)

        batch_buffer.append((buffer, None))
        if is_win: total_wins += 1
        recent_raw_rewards.append(raw_reward)

        if len(batch_buffer) >= BATCH_SIZE:
            loss = agent.update_batch(batch_buffer, entropy_coef=curr_entropy)
            batch_buffer = []
            # å¢åŠ æ‰“å°
            print(f"Ep {episode} | Loss: {loss:.4f} | Ent: {curr_entropy:.3f}")

        # å¢åŠ æ‰“å°
        if episode % 10 == 0:
            res_str = "WIN" if is_win else "LOSE"
            elapsed_time = time.time() - training_start_time
            print(
                f"Ep {episode} | RawRew: {raw_reward:.2f} | {res_str} | Wins: {total_wins} | Time: {elapsed_time / 60:.1f}m")

        # å¢åŠ æ‰“å°
        if episode % 200 == 0:
            avg_rew = np.mean(recent_raw_rewards) if recent_raw_rewards else 0
            current_session_episodes = episode - start_episode
            win_rate = total_wins / current_session_episodes * 100 if current_session_episodes > 0 else 0

            print(f"\n=== [è¶‹åŠ¿] Ep {episode} ===")
            print(f"å¹³å‡å¾—åˆ†: {avg_rew:.2f}")
            print(f"å½“å‰è¿è¡Œèƒœåœº: {total_wins}/{current_session_episodes} ({win_rate:.2f}%)")
            print(f"==========================\n")
            recent_raw_rewards = []

        # è¯„ä¼°å’Œæœ€ä½³æ¨¡å‹ä¿å­˜ (EVAL_INTERVAL = 500)
        if episode % EVAL_INTERVAL == 0:
            print(f">>> è¯„ä¼° ({EVAL_EPISODES}å±€)...")
            eval_wins = 0
            eval_rewards = []
            for _ in range(EVAL_EPISODES):
                _, raw_rew, win, _, _ = run_episode(env, agent, wt_graph, train_mode=False)
                if win: eval_wins += 1
                eval_rewards.append(raw_rew)

            curr_win_rate = eval_wins / EVAL_EPISODES
            avg_eval_reward = np.mean(eval_rewards)
            print(f">>> è¯„ä¼°èƒœç‡: {curr_win_rate * 100:.1f}% | å¹³å‡å¾—åˆ†: {avg_eval_reward:.2f}")

            # åªæœ‰å½“èƒœç‡ã€ä¸¥æ ¼å¤§äºã€‘å†å²æœ€ä½³æ—¶æ‰æ›´æ–°
            if curr_win_rate > best_win_rate:
                best_win_rate = curr_win_rate
                # === [å…³é”®] ä¿å­˜æ—¶ä¼ å…¥å½“å‰èƒœç‡ ===
                agent.save_model(MODEL_PATH, episode, win_rate=best_win_rate)
                print(f">>> æœ€ä½³æ¨¡å‹å·²æ›´æ–° (èƒœç‡ {best_win_rate:.1%} @ Ep {episode})")
            elif curr_win_rate == best_win_rate and best_win_rate > 0:
                # å…¼å®¹æ€§å¤„ç†ï¼Œå¦‚æœ best_ep ä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨å½“å‰ episode
                # === [ä¿®å¤] è¿™é‡Œä¿®æ”¹ä¸º peek_model_info ===
                ep_b, _ = peek_model_info(best_path, device)
                ep_b = ep_b or episode
                print(f">>> èƒœç‡æŒå¹³ ({best_win_rate:.1%})ï¼Œä¿ç•™åŸ Best Model (Ep {ep_b})")

        # å®‰å…¨å­˜æ¡£ (CHECKPOINT_INTERVAL = 1000)
        if episode % CHECKPOINT_INTERVAL == 0:
            ckpt_path = os.path.join(CHECKPOINT_DIR, "ckpt_latest.pt")
            agent.save_model(ckpt_path, episode)
            print(f">>> å®‰å…¨å­˜æ¡£å·²æ›´æ–°: {ckpt_path}")

    env.close()
    print("è®­ç»ƒç»“æŸï¼")


if __name__ == "__main__":
    main()